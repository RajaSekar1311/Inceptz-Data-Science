{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Prediction_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PU6hbbMTKNd",
        "outputId": "1b06b38b-be79-494a-a802-53d13399b09c"
      },
      "source": [
        "#Importing the libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "#Download the following modules once\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmf8mQkyTco1",
        "outputId": "0c97a3c5-3c79-4472-d649-44f56f544b8d"
      },
      "source": [
        "#Importing the training set\n",
        "train_data = pd.read_excel(\"/content/Data_Train.xlsx\")\n",
        "\n",
        "#Printing the top 5 rows\n",
        "print(train_data.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               STORY  SECTION\n",
            "0  But the most painful was the huge reversal in ...        3\n",
            "1  How formidable is the opposition alliance amon...        0\n",
            "2  Most Asian currencies were trading lower today...        3\n",
            "3  If you want to answer any question, click on ‘...        1\n",
            "4  In global markets, gold prices edged up today ...        3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC8D8eG9Tqzm",
        "outputId": "9e2491c8-3663-4631-b298-bbd73f1db4e2"
      },
      "source": [
        "#Printing the dataset info\n",
        "print(train_data.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7628 entries, 0 to 7627\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   STORY    7628 non-null   object\n",
            " 1   SECTION  7628 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 119.3+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "kzC_6fhxTx-7",
        "outputId": "a0a7502e-76b8-47ce-927b-8cca024f48b8"
      },
      "source": [
        "#Printing the shape of the dataset\n",
        "print(train_data.shape)\n",
        "\n",
        "Out:(7628, 2)\n",
        "\n",
        "#Printing the group by description of each category\n",
        "train_data.groupby(\"SECTION\").describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7628, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">STORY</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SECTION</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1686</td>\n",
              "      <td>1673</td>\n",
              "      <td>This story has been published from a wire agen...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2772</td>\n",
              "      <td>2731</td>\n",
              "      <td>This story has been published from a wire agen...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1924</td>\n",
              "      <td>1914</td>\n",
              "      <td>We will leave no stone unturned to make the au...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1246</td>\n",
              "      <td>1233</td>\n",
              "      <td>This story has been published from a wire agen...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        STORY                                                               \n",
              "        count unique                                                top freq\n",
              "SECTION                                                                     \n",
              "0        1686   1673  This story has been published from a wire agen...    4\n",
              "1        2772   2731  This story has been published from a wire agen...   13\n",
              "2        1924   1914  We will leave no stone unturned to make the au...    3\n",
              "3        1246   1233  This story has been published from a wire agen...   11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiVPD1VLT9Ki",
        "outputId": "76f589e9-7168-4f6e-eeff-77ab942fa9bf"
      },
      "source": [
        "# Data Cleaning \n",
        "#Removing duplicates to avoid overfitting\n",
        "train_data.drop_duplicates(inplace = True)\n",
        "\n",
        "#A punctuations string for reference (added other valid characters from the dataset)\n",
        "all_punctuations = string.punctuation + '‘’,:”][],' \n",
        "\n",
        "#Method to remove punctuation marks from the data\n",
        "def punc_remover(raw_text):\n",
        "  no_punct = \"\".join([i for i in raw_text if i not in all_punctuations])\n",
        "  return no_punct\n",
        "\n",
        "#Method to remove stopwords from the data\n",
        "def stopword_remover(no_punc_text):\n",
        "  words = no_punc_text.split()\n",
        "  no_stp_words = \" \".join([i for i in words if i not in stopwords.words('english')])\n",
        "  return no_stp_words\n",
        "\n",
        "#Method to lemmatize the words in the data\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def lem(words):\n",
        "  return \" \".join([lemmer.lemmatize(word,'v') for word in words.split()])\n",
        "\n",
        "#Method to perform a complete cleaning\n",
        "def text_cleaner(raw):\n",
        "  cleaned_text = stopword_remover(punc_remover(raw))\n",
        "  return lem(cleaned_text)\n",
        "\n",
        "#Testing the cleaner method\n",
        "text_cleaner(\"Hi!, this is a sample text to test the text cleaner method. Removes *@!#special characters%$^* and stopwords. And lemmatizes, go, going - run, ran, running\")\n",
        "\n",
        "Out: 'Hi sample text test text cleaner method Removes special character stopwords And lemmatizes go go run run run'\n",
        "\n",
        "#Applying the cleaner method to the entire data\n",
        "train_data['CLEAN_STORY'] = train_data['STORY'].apply(text_cleaner)\n",
        "\n",
        "#Checking the new dataset\n",
        "print(train_data.values) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['But the most painful was the huge reversal in fee income, unheard of among private sector lenders. Essentially, it means that Yes Bank took it for granted that fees on structured loan deals will be paid and accounted for upfront on its books. As borrowers turned defaulters, the fees tied to these loan deals fell off the cracks. Gill has now vowed to shift to a safer accounting practice of amortizing fee income rather than booking these upfront.\\n\\n\\nGill’s move to mend past ways means that there will be no nasty surprises in the future. This is good news considering that investors love a clean image and loathe uncertainties.\\n\\n\\nBut there is no gain without pain and the promise of a strong and stable balance sheet comes with some sacrifices as well. Investors will have to give up the hopes of phenomenal growth, a promise made by Kapoor.'\n",
            "  3\n",
            "  'But painful huge reversal fee income unheard among private sector lenders Essentially mean Yes Bank take grant fee structure loan deal pay account upfront book As borrowers turn defaulters fee tie loan deal fell crack Gill vow shift safer account practice amortize fee income rather book upfront Gills move mend past ways mean nasty surprise future This good news consider investors love clean image loathe uncertainties But gain without pain promise strong stable balance sheet come sacrifice well Investors give hop phenomenal growth promise make Kapoor']\n",
            " ['How formidable is the opposition alliance among Congress, Jharkhand Mukti Morcha (JMM) and Jharkhand Vikas Morcha (Prajatantrik)?'\n",
            "  0\n",
            "  'How formidable opposition alliance among Congress Jharkhand Mukti Morcha JMM Jharkhand Vikas Morcha Prajatantrik']\n",
            " ['Most Asian currencies were trading lower today. South Korean won was down 0.4%, China renminbi 0.23%, China Offshore 0.15%, Malaysian ringgit 0.12%, Indonesian rupiah 0.11%, Taiwan dollar 0.06%. However, Japanese yen was up 0.32%.\\n\\n\\nThe dollar index, which measures the US currency’s strength against major currencies, was trading at 97.26, down 0.14% from its previous close of 97.395.'\n",
            "  3\n",
            "  'Most Asian currencies trade lower today South Korean 04 China renminbi 023 China Offshore 015 Malaysian ringgit 012 Indonesian rupiah 011 Taiwan dollar 006 However Japanese yen 032 The dollar index measure US currencys strength major currencies trade 9726 014 previous close 97395']\n",
            " ...\n",
            " ['The database has been created after bringing together criminal records of the state police, the prisons department and the GRP that guards the railway network in the state.\\n\\n\\nA senior official involved in developing the app said the database will help policeman on ground by deploying techniques like face recognition, text search, biometric records analysis, phonetic search, artificial intelligence (AI) and gang analysis to “zero in on the criminal\" in a quick and targeted manner.'\n",
            "  1\n",
            "  'The database create bring together criminal record state police prisons department GRP guard railway network state A senior official involve develop app say database help policeman grind deploy techniques like face recognition text search biometric record analysis phonetic search artificial intelligence AI gang analysis “zero criminal quick target manner']\n",
            " ['The state, which has had an uneasy relationship with the mainland since the days of the late pro-independence leader Angami Zapu Phizo, is wary of National Democratic Alliance (NDA) promises because of the Naga Peace Accord, 2015, held up by slow progress of talks and the controversial Citizenship (Amendment) Bill, 2016. These, along with the aversion of the people of the Christian-majority state toward the BJP’s agenda to promote Hindutva and the uniform civil code, are the major issues in Nagaland.\\n\\n\\nThe direct contest in Nagaland, where money power has fuelled political corruption, is between the ‘hand’ and the ‘globe’, the election symbols of the Congress and the NDPP, respectively. Many believe the BJP-NDPP alliance has an edge because the state tends to send the ruling party candidate to the Lok Sabha. “If you don’t have support in Delhi, any state project will run into roadblocks,\" said Kathi Chishi, secretary of Toka MPCS, an organization that provides financing for rural livelihood.\\n\\n\\nThe NDPP’s candidate is Tokheho Yepthomi, the current member of parliament, who won the seat last year after Neiphiu Rio vacated it to contest the assembly elections. The Congress candidate is K.L. Chishi, former chief minister and a veteran Naga politician. Conrad Sangma’s National People’s Party has fielded a candidate, while the Naga People’s Front is backing its arch rival, the Congress, because it is against the BJP’s majoritarian Hindutva agenda and believes Congress is the party with a secular plank. An independent, M.M. Thromwa Konyak, is also in the fray. The BJP released its manifesto on Monday, restating its commitment to passing the citizenship amendment bill, which had sparked violence in the North-East. However, Yepthomi said on Friday, that after “the government is formed, the NDA partners will meet and then discuss. And what will be best for all stakeholders will be decided on that basis.\"'\n",
            "  0\n",
            "  'The state uneasy relationship mainland since days late proindependence leader Angami Zapu Phizo wary National Democratic Alliance NDA promise Naga Peace Accord 2015 hold slow progress talk controversial Citizenship Amendment Bill 2016 These along aversion people Christianmajority state toward BJPs agenda promote Hindutva uniform civil code major issue Nagaland The direct contest Nagaland money power fuel political corruption hand globe election symbols Congress NDPP respectively Many believe BJPNDPP alliance edge state tend send rule party candidate Lok Sabha “If dont support Delhi state project run roadblocks say Kathi Chishi secretary Toka MPCS organization provide finance rural livelihood The NDPPs candidate Tokheho Yepthomi current member parliament seat last year Neiphiu Rio vacate contest assembly elections The Congress candidate KL Chishi former chief minister veteran Naga politician Conrad Sangmas National Peoples Party field candidate Naga Peoples Front back arch rival Congress BJPs majoritarian Hindutva agenda believe Congress party secular plank An independent MM Thromwa Konyak also fray The BJP release manifesto Monday restate commitment pass citizenship amendment bill spark violence NorthEast However Yepthomi say Friday “the government form NDA partner meet discuss And best stakeholders decide basis']\n",
            " ['Virus stars Kunchacko Boban, Tovino Thomas, Indrajith Sukumaran, Asif Ali, Soubin Shahir, Poornima Indrajith, Sreenath Bhasi, Rima Kallingal, Remya Nambeesan, Joju George, Dileesh Pothan, Senthil Krishna, Rahman, Revathy, Asha Kelunni, Parvathy Thiruvothu, Indrans and Madonna Sebastian'\n",
            "  2\n",
            "  'Virus star Kunchacko Boban Tovino Thomas Indrajith Sukumaran Asif Ali Soubin Shahir Poornima Indrajith Sreenath Bhasi Rima Kallingal Remya Nambeesan Joju George Dileesh Pothan Senthil Krishna Rahman Revathy Asha Kelunni Parvathy Thiruvothu Indrans Madonna Sebastian']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rAJZ-mCUjHl",
        "outputId": "a77a69fc-810c-416c-929f-d96c038baf2c"
      },
      "source": [
        "#Data Preprocessing: Count Vectors and TF-IDF Vectors\n",
        "#Creating Count vectors\n",
        "#Importing sklearn’s Countvectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Creating a bag-of-words dictionary of words from the data\n",
        "bow_dictionary = CountVectorizer().fit(train_data['CLEAN_STORY'])\n",
        "\n",
        "#Total number of words in the bow_dictionary\n",
        "len(bow_dictionary.vocabulary_)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuWZ9TS9U4Nn",
        "outputId": "2ec27792-9d65-4f17-c51a-2ec424384ac8"
      },
      "source": [
        "#Using the bow_dictionary to create count vectors for the cleaned data.\n",
        "bow = bow_dictionary.transform(train_data['CLEAN_STORY'])\n",
        "\n",
        "#Printing the shape of the bag of words model\n",
        "print(bow.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7551, 35189)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uly7og1pU-1E"
      },
      "source": [
        "#Creating TF-IDF Vectors\n",
        "#Importing TfidfTransformer from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#Fitting the bag of words data to the TF-IDF transformer\n",
        "tfidf_transformer = TfidfTransformer().fit(bow)\n",
        "\n",
        "#Transforming the bag of words model to TF-IDF vectors\n",
        "storytfidf = tfidf_transformer.transform(bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ur1D0vGVGaC"
      },
      "source": [
        "#Training The Classifier\n",
        "#Creating a Multinomial Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Fitting the training data to the classifier\n",
        "classifier = MultinomialNB().fit(storytfidf, train_data['SECTION'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks6y9yhhZcBU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baCtpAWJVLpb",
        "outputId": "be2e453f-b5ba-4bb4-8d3f-5f919e965acc"
      },
      "source": [
        "#Predicting For The Test Set\n",
        "#Importing and cleaning the test data\n",
        "test_data = pd.read_excel(\"/content/Data_Test.xlsx\")\n",
        "test_data['CLEAN_STORY'] = test_data['STORY'].apply(text_cleaner)\n",
        "\n",
        "#Printing the cleaned data\n",
        "print(test_data.values)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['2019 will see gadgets like gaming smartphones and wearable medical devices lifting the user experience to a whole new level\\n\\n\\nmint-india-wire consumer technologyconsumer technology trends in New Yeartech gadgetsFoldable phonesgaming smartphoneswearable medical devicestechnology\\n\\n\\nNew Delhi: Gadgets have become an integral part of our lives with most of us relying on some form of factor to communicate, commute, work, be informed or entertained. Year 2019 will see some gadgets lifting the user experience to a whole new level. Here’s what we can expect to see:\\n\\n\\nSmartphones with foldable screens: Foldable phones are finally moving from the concept stage to commercial launches. They are made up of organic light-emitting diode (OLED) panels with higher plastic substrates, allowing them to be bent without damage.\\n\\n\\nUS-based display maker Royole Corp’s foldable phone, FlexPai, has already arrived in select markets, while Samsung’s unnamed foldable phone is expected sometime next year. Samsung’s smartphone chief executive officer D.J. Koh has said they will make a million units of it. LG, too, is expected to display a foldable phone next year. Meanwhile Apple, Nokia, Lenovo and Huawei have also been working on foldable phones, reportedly.\\n\\n\\neSIM: Very soon your smartphone won’t need a physical SIM card anymore. The eSIM technology, already used by Apple in its iPhones and Apple Watch, replaces the physical SIM with a virtually embedded chip on the motherboard. eSIMs support multiple mobile operators and can be programmed to switch services.'\n",
            "  '2019 see gadgets like game smartphones wearable medical devices lift user experience whole new level mintindiawire consumer technologyconsumer technology trend New Yeartech gadgetsFoldable phonesgaming smartphoneswearable medical devicestechnology New Delhi Gadgets become integral part live us rely form factor communicate commute work inform entertain Year 2019 see gadgets lift user experience whole new level Heres expect see Smartphones foldable screen Foldable phone finally move concept stage commercial launch They make organic lightemitting diode OLED panel higher plastic substrates allow bend without damage USbased display maker Royole Corps foldable phone FlexPai already arrive select market Samsungs unnamed foldable phone expect sometime next year Samsungs smartphone chief executive officer DJ Koh say make million units LG expect display foldable phone next year Meanwhile Apple Nokia Lenovo Huawei also work foldable phone reportedly eSIM Very soon smartphone wont need physical SIM card anymore The eSIM technology already use Apple iPhones Apple Watch replace physical SIM virtually embed chip motherboard eSIMs support multiple mobile operators program switch service']\n",
            " ['It has also unleashed a wave of changes in the MCU that will make sure its future is a lot different than its past\\n\\n\\nKevin Feige had signalled diversity and more representation in the post-phase 3 MCU and Endgame does a lot to showcase the initiative'\n",
            "  'It also unleash wave change MCU make sure future lot different past Kevin Feige signal diversity representation postphase 3 MCU Endgame lot showcase initiative']\n",
            " ['It can be confusing to pick the right smartphone for yourself, so we have segregated the top smartphones under Rs 20,000 according to their strengths.\\n\\n\\nThe best smartphones under ₹20,000 categorised according to performance, camera, design and battery life\\n\\n\\nmint-india-wire phones under Rs 20000Poco F1Realme U1Redmi Note 6 Prorealme 2 proHonor PlayNokia 7.1Nova 3iAsus Zenfone Max Pro M1\\n\\n\\nGone are the days when you had to shell out big buck for buying smartphones with premium features. Technology has become more accessible recently and the biggest example of that lies in the sub-Rs 20,000 category—you get good performance, design and even software at a reasonable price.\\n\\n\\nIt can be confusing to pick the right smartphone for you, however, given the amount of variety that lies in the segment. So we have segregated the top smartphones under ₹ 20,000 according to their strengths, so you can pick the one that suits you best.\\n\\n\\nThis phone actually lies just north of the ₹ 20,000 price point. But if you have an HDFC debit or credit card, you can purchase the lowest spec variant with 6GB RAM and 64GB internal storage for as low as ₹ 19,999, making it the cheapest smartphone to run a Qualcomm Snapdragon 845 SoC. There’s not a lot to not like about this phone—it has the fastest processor Qualcomm has to offer, some thermal trickery to keep your smartphone cool during intense gaming sessions, a very good camera and some durable plastic that doesn’t shatter or pick up scratches.\\n\\n\\nIt even gets a modded version of the MIUI with an app drawer that allows you colour code your applications.'\n",
            "  'It confuse pick right smartphone segregate top smartphones Rs 20000 accord strengths The best smartphones ₹20000 categorise accord performance camera design battery life mintindiawire phone Rs 20000Poco F1Realme U1Redmi Note 6 Prorealme 2 proHonor PlayNokia 71Nova 3iAsus Zenfone Max Pro M1 Gone days shell big buck buy smartphones premium feature Technology become accessible recently biggest example lie subRs 20000 category—you get good performance design even software reasonable price It confuse pick right smartphone however give amount variety lie segment So segregate top smartphones ₹ 20000 accord strengths pick one suit best This phone actually lie north ₹ 20000 price point But HDFC debit credit card purchase lowest spec variant 6GB RAM 64GB internal storage low ₹ 19999 make cheapest smartphone run Qualcomm Snapdragon 845 SoC Theres lot like phone—it fastest processor Qualcomm offer thermal trickery keep smartphone cool intense game sessions good camera durable plastic doesnt shatter pick scratch It even get modded version MIUI app drawer allow colour code applications']\n",
            " ...\n",
            " ['On the photography front, the Note 5 Pro features a 12MP + 15MP dual rear camera setup, sporting an aperture of f/2.2, while on the front it has a 20MP selfie camera.\\n\\n\\nXiaomi Redmi Note 5 ProRedmi Note 5 Pro price cutRedmi Note 5 Pro price indiaRedmi Note 5 Pro specificationsRedmi Note 5 Pro saleRedmi Note 5 Pro units soldRedmi Note 5 Pro review'\n",
            "  'On photography front Note 5 Pro feature 12MP 15MP dual rear camera setup sport aperture f22 front 20MP selfie camera Xiaomi Redmi Note 5 ProRedmi Note 5 Pro price cutRedmi Note 5 Pro price indiaRedmi Note 5 Pro specificationsRedmi Note 5 Pro saleRedmi Note 5 Pro units soldRedmi Note 5 Pro review']\n",
            " ['UDAY mandated that discoms bring the gap between average revenue and average costs to zero. Here, too, there has been progress with the gap reducing (from ₹0.60/kWh in 2015-16 to ₹0.17/kWh in 2017-18), but 21 of 26 states are still unlikely to meet the target.\\n\\n\\nAccording to a National Institute of Public Finance and Policy study, this data suggests that the UDAY scheme is failing to turn around the power sector. The authors also find that discoms remain plagued by operational inefficiencies such as lack of effective billing procedures, poor measurement of power consumption, and ineffective monitoring of power theft.\\n\\n\\nTaken together, data shows that the NDA has built on the previous government’s work to get more households electrified—but quality of access still remains an issue.\\n\\n\\nWhile the government has tried to address this by improving the financial health of discoms, much more needs to be done. The inability of successive governments to revive discom fortunes could have important ramifications for India’s development.\\n\\n\\nThe World Bank estimates India’s electricity demand to treble by 2040. Addressing this rising demand will be critical for the development agenda of whichever party is elected to form the next government.'\n",
            "  'UDAY mandate discoms bring gap average revenue average cost zero Here progress gap reduce ₹060kWh 201516 ₹017kWh 201718 21 26 state still unlikely meet target According National Institute Public Finance Policy study data suggest UDAY scheme fail turn around power sector The author also find discoms remain plague operational inefficiencies lack effective bill procedures poor measurement power consumption ineffective monitor power theft Taken together data show NDA build previous governments work get households electrified—but quality access still remain issue While government try address improve financial health discoms much need do The inability successive governments revive discom fortunes could important ramifications Indias development The World Bank estimate Indias electricity demand treble 2040 Addressing rise demand critical development agenda whichever party elect form next government']\n",
            " ['Ripple also helps bank customers send money to people in many emerging markets including Mexico, India, and Thailand to increase their share of “this large and growing market\". What’s next? “Ripple is moving beyond blockchain, and connecting networks so that we can move money across networks. Again this is open-source and lightweight so it becomes easy to transfer money across networks. So we are building the ecosystem for networks to connect with each other and in our view globalization will be completed when data, goods and money flow seamlessly. That’s the way we think of it as an internet of value when the whole world gets connected through payment systems,\" Gupta said.'\n",
            "  'Ripple also help bank customers send money people many emerge market include Mexico India Thailand increase share “this large grow market Whats next “Ripple move beyond blockchain connect network move money across network Again opensource lightweight become easy transfer money across network So build ecosystem network connect view globalization complete data goods money flow seamlessly Thats way think internet value whole world get connect payment systems Gupta say']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNGilOHiVc8r"
      },
      "source": [
        "#Creating A Pipeline To Pre-Process The Data & Initialise The Classifier\n",
        "#Importing the Pipeline module from sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#Initializing the pipeline with necessary transformations and the required classifier\n",
        "pipe = Pipeline([\n",
        "('bow', CountVectorizer()),\n",
        "('tfidf', TfidfTransformer()),\n",
        "('classifier', MultinomialNB())])\n",
        "\n",
        "#Fitting the training data to the pipeline\n",
        "pipe.fit(train_data['CLEAN_STORY'], train_data['SECTION'])\n",
        "\n",
        "#Predicting the SECTION\n",
        "test_preds_mnb = pipe.predict(test_data['CLEAN_STORY'])\n",
        "\n",
        "#Writing the predictions to an excel sheet\n",
        "pd.DataFrame(test_preds_mnb, columns = ['SECTION']).to_excel(\"NBpredictions.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTRwvtMyXzap"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPA5UtYFRaF8"
      },
      "source": [
        "### Try out Logistic Regression\n",
        "\n",
        "The logistic regression model is actually a statistical model developed by statistician\n",
        "David Cox in 1958. It is also known as the logit or logistic model since it uses the\n",
        "logistic (popularly also known as sigmoid) mathematical function to estimate the\n",
        "parameter values. These are the coefficients of all our features such that the overall loss\n",
        "is minimized when predicting the outcome—"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGHQErnMRaF9",
        "outputId": "4eb2b6b9-5faf-4cbe-e9ac-bd821b0d417c"
      },
      "source": [
        "%%time\n",
        "\n",
        "#Training The Classifier\n",
        "#Creating a Multinomial Naive Bayes Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Fitting the training data to the classifier\n",
        "classifier = LogisticRegression().fit(storytfidf, train_data['SECTION'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.47 s, sys: 4.79 s, total: 9.26 s\n",
            "Wall time: 4.81 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkfEC0bKd21u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igAotpvVaYmn"
      },
      "source": [
        "#Creating A Pipeline To Pre-Process The Data & Initialise The Classifier\n",
        "#Importing the Pipeline module from sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#Initializing the pipeline with necessary transformations and the required classifier\n",
        "pipe = Pipeline([\n",
        "('bow', CountVectorizer()),\n",
        "('tfidf', TfidfTransformer()),\n",
        "('classifier',LogisticRegression() )])\n",
        "\n",
        "#Fitting the training data to the pipeline\n",
        "pipe.fit(train_data['CLEAN_STORY'], train_data['SECTION'])\n",
        "\n",
        "#Predicting the SECTION\n",
        "test_preds_mnb = pipe.predict(test_data['CLEAN_STORY'])\n",
        "\n",
        "#Writing the predictions to an excel sheet\n",
        "pd.DataFrame(test_preds_mnb, columns = ['SECTION']).to_excel(\"LogRegpredictions.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoOoEAiXRaGH"
      },
      "source": [
        "### Try out Random Forest\n",
        "\n",
        "Decision trees are a family of supervised machine learning algorithms that can represent\n",
        "and interpret sets of rules automatically from the underlying data. They use metrics like\n",
        "information gain and gini-index to build the tree. However, a major drawback of decision\n",
        "trees is that since they are non-parametric, the more data there is, greater the depth of\n",
        "the tree. We can end up with really huge and deep trees that are prone to overfitting. The\n",
        "model might work really well on training data, but instead of learning, it just memorizes\n",
        "all the training samples and builds very specific rules to them. Hence, it performs really\n",
        "poorly on the test data. Random forests try to tackle this problem.\n",
        "\n",
        "A random forest is a meta-estimator or an ensemble model that fits a number of\n",
        "decision tree classifiers on various sub-samples of the dataset and uses averaging to\n",
        "improve the predictive accuracy and control over-fitting. The sub-sample size is always\n",
        "the same as the original input sample size, but the samples are drawn with replacement\n",
        "(bootstrap samples). In random forests, all the trees are trained in parallel (bagging\n",
        "model/bootstrap aggregation). Besides this, each tree in the ensemble is built from a\n",
        "sample drawn with replacement (i.e., a bootstrap sample) from the training set. Also,\n",
        "when splitting a node during the construction of the tree, the split that is chosen is no\n",
        "longer the best split among all features. Instead, the split that is picked is the best split\n",
        "among a random subset of the features. T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlN-nzuMbd7g",
        "outputId": "6ed00b03-b5dc-47c9-bf08-787628f89da9"
      },
      "source": [
        "%%time\n",
        "\n",
        "#Training The Classifier\n",
        "#Creating a Multinomial Naive Bayes Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Fitting the training data to the classifier\n",
        "classifier = RandomForestClassifier().fit(storytfidf, train_data['SECTION'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.27 s, sys: 25.6 ms, total: 8.29 s\n",
            "Wall time: 8.27 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M94mlMI8b4K0"
      },
      "source": [
        "#Creating A Pipeline To Pre-Process The Data & Initialise The Classifier\n",
        "#Importing the Pipeline module from sklearn\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#Initializing the pipeline with necessary transformations and the required classifier\n",
        "pipe = Pipeline([\n",
        "('bow', CountVectorizer()),\n",
        "('tfidf', TfidfTransformer()),\n",
        "('classifier',RandomForestClassifier())])\n",
        "\n",
        "#Fitting the training data to the pipeline\n",
        "pipe.fit(train_data['CLEAN_STORY'], train_data['SECTION'])\n",
        "\n",
        "#Predicting the SECTION\n",
        "test_preds_mnb = pipe.predict(test_data['CLEAN_STORY'])\n",
        "\n",
        "#Writing the predictions to an excel sheet\n",
        "pd.DataFrame(test_preds_mnb, columns = ['SECTION']).to_excel(\"RanForpredictions.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjpEffF4wjQD"
      },
      "source": [
        "#Newer Supervised Deep Learning Modelsimport gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0U3crLFd4kD"
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZKo9qTo8BKR"
      },
      "source": [
        "### Build Train and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6nRZNj66mLr"
      },
      "source": [
        "# build train and test datasets\n",
        "sentiments= train_data['SECTION'].values\n",
        "reviews = train_data['STORY'].values\n",
        "\n",
        "train_reviews = train_data['STORY']\n",
        "train_sentiments = train_data['SECTION']\n",
        "\n",
        "test_reviews = test_data['STORY']"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCOEfEGD8ILv"
      },
      "source": [
        "### Text Wrangling & Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstzlK0g7U20"
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm.tqdm(docs):\n",
        "    doc = strip_html_tags(doc)\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = remove_accented_chars(doc)\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()  \n",
        "    norm_docs.append(doc)\n",
        "  \n",
        "  return norm_docs"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD5bUYb67ZCp",
        "outputId": "ab5c7c55-edfd-4c4b-c1d1-c295708b7b89"
      },
      "source": [
        "%%time\n",
        "\n",
        "norm_train_reviews = pre_process_corpus(train_reviews)\n",
        "norm_test_reviews = pre_process_corpus(test_reviews)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7551/7551 [00:02<00:00, 3383.51it/s]\n",
            "100%|██████████| 2748/2748 [00:00<00:00, 3476.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.98 s, sys: 33.5 ms, total: 3.01 s\n",
            "Wall time: 3.03 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwxiMn008PGV"
      },
      "source": [
        "### Traditional Supervised Machine Learning Models\n",
        "Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op_oAXLY7eJz",
        "outputId": "1d4471e6-2d9e-415a-dadf-752d45f62504"
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
        "\n",
        "\n",
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.3 s, sys: 107 ms, total: 6.41 s\n",
            "Wall time: 6.42 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSxpJB-x7huQ",
        "outputId": "83bf537c-1930-4824-b343-50eaf4fe1876"
      },
      "source": [
        "%%time\n",
        "\n",
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.1 s, sys: 4.38 ms, total: 1.1 s\n",
            "Wall time: 1.11 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2KFcm1672O8",
        "outputId": "13b83f30-8db9-4e90-bb09-b5dc3db7c483"
      },
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW model:> Train features shape: (7551, 31634)  Test features shape: (2748, 31634)\n",
            "TFIDF model:> Train features shape: (7551, 31634)  Test features shape: (2748, 31634)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyJtktE43Saq"
      },
      "source": [
        "\n",
        "###Prediction class label encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzHyU6uc3Rr1"
      },
      "source": [
        "# converting y data into categorical (one-hot encoding)\n",
        "y_train = to_categorical(train_sentiments)\n",
        "#y_test = to_categorical(y_test)\n",
        "\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [nltk.word_tokenize(text)\n",
        "                       for text in norm_train_reviews]\n",
        "y_train = le.fit_transform(y_train)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text)\n",
        "                      for text in norm_test_reviews]\n",
        "#y_test = le.fit_transform(test_sentiments)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b-fo7J-3eE4"
      },
      "source": [
        "###Feature Engineering with word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-87UPA43gY9"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRoF2muq3tte",
        "outputId": "bd9960ac-dfc7-4aa2-f381-78a7cc61d19a"
      },
      "source": [
        "%%time\n",
        "# build word2vec model\n",
        "w2v_num_features = 300\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
        "                                   min_count=10, workers=4, iter=5)    "
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-11 07:59:50,977 : INFO : collecting all words and their counts\n",
            "2021-04-11 07:59:50,983 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-04-11 07:59:51,185 : INFO : collected 40121 word types from a corpus of 816390 raw words and 7551 sentences\n",
            "2021-04-11 07:59:51,187 : INFO : Loading a fresh vocabulary\n",
            "2021-04-11 07:59:51,540 : INFO : effective_min_count=10 retains 6820 unique words (16% of original 40121, drops 33301)\n",
            "2021-04-11 07:59:51,543 : INFO : effective_min_count=10 leaves 741453 word corpus (90% of original 816390, drops 74937)\n",
            "2021-04-11 07:59:51,569 : INFO : deleting the raw counts dictionary of 40121 items\n",
            "2021-04-11 07:59:51,573 : INFO : sample=0.001 downsamples 36 most-common words\n",
            "2021-04-11 07:59:51,576 : INFO : downsampling leaves estimated 568146 word corpus (76.6% of prior 741453)\n",
            "2021-04-11 07:59:51,605 : INFO : estimated required memory for 6820 words and 300 dimensions: 19778000 bytes\n",
            "2021-04-11 07:59:51,607 : INFO : resetting layer weights\n",
            "2021-04-11 07:59:53,009 : INFO : training model with 4 workers on 6820 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
            "2021-04-11 07:59:54,039 : INFO : EPOCH 1 - PROGRESS: at 17.53% examples, 94203 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 07:59:55,070 : INFO : EPOCH 1 - PROGRESS: at 36.82% examples, 100649 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 07:59:56,091 : INFO : EPOCH 1 - PROGRESS: at 55.69% examples, 103038 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 07:59:57,149 : INFO : EPOCH 1 - PROGRESS: at 75.25% examples, 103328 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 07:59:58,219 : INFO : EPOCH 1 - PROGRESS: at 94.52% examples, 103271 words/s, in_qsize 5, out_qsize 0\n",
            "2021-04-11 07:59:58,274 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 07:59:58,283 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 07:59:58,328 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 07:59:58,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 07:59:58,360 : INFO : EPOCH - 1 : training on 816390 raw words (567933 effective words) took 5.3s, 106259 effective words/s\n",
            "2021-04-11 07:59:59,381 : INFO : EPOCH 2 - PROGRESS: at 17.53% examples, 95672 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:00,441 : INFO : EPOCH 2 - PROGRESS: at 35.77% examples, 96651 words/s, in_qsize 8, out_qsize 3\n",
            "2021-04-11 08:00:01,501 : INFO : EPOCH 2 - PROGRESS: at 55.95% examples, 101311 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-11 08:00:02,528 : INFO : EPOCH 2 - PROGRESS: at 74.22% examples, 101153 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:03,675 : INFO : EPOCH 2 - PROGRESS: at 95.82% examples, 102688 words/s, in_qsize 4, out_qsize 0\n",
            "2021-04-11 08:00:03,687 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 08:00:03,735 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 08:00:03,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 08:00:03,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 08:00:03,758 : INFO : EPOCH - 2 : training on 816390 raw words (568075 effective words) took 5.4s, 105476 effective words/s\n",
            "2021-04-11 08:00:04,790 : INFO : EPOCH 3 - PROGRESS: at 17.53% examples, 94168 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:05,819 : INFO : EPOCH 3 - PROGRESS: at 36.82% examples, 100633 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:06,861 : INFO : EPOCH 3 - PROGRESS: at 55.69% examples, 102390 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:07,953 : INFO : EPOCH 3 - PROGRESS: at 75.25% examples, 102068 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:09,082 : INFO : EPOCH 3 - PROGRESS: at 95.82% examples, 102457 words/s, in_qsize 4, out_qsize 0\n",
            "2021-04-11 08:00:09,094 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 08:00:09,104 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 08:00:09,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 08:00:09,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 08:00:09,142 : INFO : EPOCH - 3 : training on 816390 raw words (568318 effective words) took 5.4s, 105716 effective words/s\n",
            "2021-04-11 08:00:10,154 : INFO : EPOCH 4 - PROGRESS: at 16.33% examples, 89041 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:11,219 : INFO : EPOCH 4 - PROGRESS: at 35.77% examples, 96617 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:12,228 : INFO : EPOCH 4 - PROGRESS: at 54.62% examples, 100709 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:13,292 : INFO : EPOCH 4 - PROGRESS: at 74.22% examples, 101457 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:14,372 : INFO : EPOCH 4 - PROGRESS: at 94.52% examples, 102903 words/s, in_qsize 5, out_qsize 0\n",
            "2021-04-11 08:00:14,449 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 08:00:14,493 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 08:00:14,514 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 08:00:14,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 08:00:14,547 : INFO : EPOCH - 4 : training on 816390 raw words (567692 effective words) took 5.4s, 105206 effective words/s\n",
            "2021-04-11 08:00:15,565 : INFO : EPOCH 5 - PROGRESS: at 16.33% examples, 88837 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:16,593 : INFO : EPOCH 5 - PROGRESS: at 35.77% examples, 98263 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:17,596 : INFO : EPOCH 5 - PROGRESS: at 54.62% examples, 102078 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:18,625 : INFO : EPOCH 5 - PROGRESS: at 74.22% examples, 103345 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:19,632 : INFO : EPOCH 5 - PROGRESS: at 92.12% examples, 103191 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-11 08:00:19,892 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-11 08:00:19,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-11 08:00:19,931 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-11 08:00:19,939 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-11 08:00:19,940 : INFO : EPOCH - 5 : training on 816390 raw words (568056 effective words) took 5.4s, 105517 effective words/s\n",
            "2021-04-11 08:00:19,942 : INFO : training on a 4081950 raw words (2840074 effective words) took 26.9s, 105456 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 54.4 s, sys: 131 ms, total: 54.6 s\n",
            "Wall time: 29 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elq4bV2n32x7"
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWEih1we37Ef"
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAMA2sj84F9n",
        "outputId": "618fa94c-c92d-4a1a-d03a-c8416695359e"
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (7551, 300)  Test features shape: (2748, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx6w_tCt4XKa"
      },
      "source": [
        "from keras.layers import BatchNormalization"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLucyRAp4iGn"
      },
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,), kernel_initializer='he_normal'))\n",
        "    dnn_model.add(BatchNormalization())\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256, kernel_initializer='he_normal'))\n",
        "    dnn_model.add(BatchNormalization())\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256, kernel_initializer='he_normal'))\n",
        "    dnn_model.add(BatchNormalization())\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(4))\n",
        "    dnn_model.add(Activation('softmax'))\n",
        "\n",
        "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ0CpwVq4meH"
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itNNBcSS4ys4",
        "outputId": "805b68cf-9aa8-41c7-f326-11030398545f"
      },
      "source": [
        "w2v_dnn.summary()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 4)                 1028      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 356,356\n",
            "Trainable params: 354,308\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ZdpICs45LB",
        "outputId": "bc5607f8-37f8-4c05-bfb2-d60ecf5e4c03"
      },
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, to_categorical(train_sentiments), epochs=50, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "68/68 [==============================] - 2s 16ms/step - loss: 0.4161 - accuracy: 0.8540 - val_loss: 0.2081 - val_accuracy: 0.9405\n",
            "Epoch 2/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1574 - accuracy: 0.9485 - val_loss: 0.1592 - val_accuracy: 0.9471\n",
            "Epoch 3/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1479 - accuracy: 0.9512 - val_loss: 0.1390 - val_accuracy: 0.9497\n",
            "Epoch 4/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1479 - accuracy: 0.9472 - val_loss: 0.1318 - val_accuracy: 0.9563\n",
            "Epoch 5/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1409 - accuracy: 0.9507 - val_loss: 0.1343 - val_accuracy: 0.9537\n",
            "Epoch 6/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1279 - accuracy: 0.9560 - val_loss: 0.1354 - val_accuracy: 0.9550\n",
            "Epoch 7/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1146 - accuracy: 0.9596 - val_loss: 0.1407 - val_accuracy: 0.9563\n",
            "Epoch 8/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1203 - accuracy: 0.9591 - val_loss: 0.1313 - val_accuracy: 0.9616\n",
            "Epoch 9/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1254 - accuracy: 0.9590 - val_loss: 0.1282 - val_accuracy: 0.9563\n",
            "Epoch 10/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1194 - accuracy: 0.9581 - val_loss: 0.1305 - val_accuracy: 0.9603\n",
            "Epoch 11/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1101 - accuracy: 0.9633 - val_loss: 0.1371 - val_accuracy: 0.9511\n",
            "Epoch 12/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1042 - accuracy: 0.9641 - val_loss: 0.1345 - val_accuracy: 0.9563\n",
            "Epoch 13/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.1060 - accuracy: 0.9625 - val_loss: 0.1508 - val_accuracy: 0.9497\n",
            "Epoch 14/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0993 - accuracy: 0.9691 - val_loss: 0.1225 - val_accuracy: 0.9577\n",
            "Epoch 15/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.1012 - accuracy: 0.9632 - val_loss: 0.1422 - val_accuracy: 0.9511\n",
            "Epoch 16/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0930 - accuracy: 0.9666 - val_loss: 0.1327 - val_accuracy: 0.9524\n",
            "Epoch 17/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0942 - accuracy: 0.9649 - val_loss: 0.1192 - val_accuracy: 0.9603\n",
            "Epoch 18/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0975 - accuracy: 0.9665 - val_loss: 0.1207 - val_accuracy: 0.9577\n",
            "Epoch 19/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0853 - accuracy: 0.9688 - val_loss: 0.1199 - val_accuracy: 0.9590\n",
            "Epoch 20/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0889 - accuracy: 0.9672 - val_loss: 0.1251 - val_accuracy: 0.9603\n",
            "Epoch 21/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0832 - accuracy: 0.9713 - val_loss: 0.1296 - val_accuracy: 0.9590\n",
            "Epoch 22/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0962 - accuracy: 0.9662 - val_loss: 0.1339 - val_accuracy: 0.9590\n",
            "Epoch 23/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0884 - accuracy: 0.9702 - val_loss: 0.1384 - val_accuracy: 0.9630\n",
            "Epoch 24/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0822 - accuracy: 0.9733 - val_loss: 0.1310 - val_accuracy: 0.9590\n",
            "Epoch 25/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0814 - accuracy: 0.9720 - val_loss: 0.1283 - val_accuracy: 0.9616\n",
            "Epoch 26/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0839 - accuracy: 0.9679 - val_loss: 0.1351 - val_accuracy: 0.9563\n",
            "Epoch 27/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0840 - accuracy: 0.9693 - val_loss: 0.1296 - val_accuracy: 0.9616\n",
            "Epoch 28/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0720 - accuracy: 0.9743 - val_loss: 0.1319 - val_accuracy: 0.9577\n",
            "Epoch 29/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0791 - accuracy: 0.9715 - val_loss: 0.1410 - val_accuracy: 0.9550\n",
            "Epoch 30/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0789 - accuracy: 0.9714 - val_loss: 0.1510 - val_accuracy: 0.9577\n",
            "Epoch 31/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0702 - accuracy: 0.9719 - val_loss: 0.1468 - val_accuracy: 0.9563\n",
            "Epoch 32/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0710 - accuracy: 0.9729 - val_loss: 0.1408 - val_accuracy: 0.9603\n",
            "Epoch 33/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0606 - accuracy: 0.9789 - val_loss: 0.1398 - val_accuracy: 0.9616\n",
            "Epoch 34/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 0.1482 - val_accuracy: 0.9590\n",
            "Epoch 35/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0636 - accuracy: 0.9765 - val_loss: 0.1401 - val_accuracy: 0.9603\n",
            "Epoch 36/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0694 - accuracy: 0.9745 - val_loss: 0.1362 - val_accuracy: 0.9643\n",
            "Epoch 37/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0688 - accuracy: 0.9742 - val_loss: 0.1657 - val_accuracy: 0.9550\n",
            "Epoch 38/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0587 - accuracy: 0.9791 - val_loss: 0.1398 - val_accuracy: 0.9616\n",
            "Epoch 39/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0625 - accuracy: 0.9761 - val_loss: 0.1489 - val_accuracy: 0.9563\n",
            "Epoch 40/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0680 - accuracy: 0.9758 - val_loss: 0.1253 - val_accuracy: 0.9630\n",
            "Epoch 41/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0673 - accuracy: 0.9785 - val_loss: 0.1419 - val_accuracy: 0.9590\n",
            "Epoch 42/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0578 - accuracy: 0.9796 - val_loss: 0.1632 - val_accuracy: 0.9484\n",
            "Epoch 43/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0742 - accuracy: 0.9730 - val_loss: 0.1390 - val_accuracy: 0.9603\n",
            "Epoch 44/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0566 - accuracy: 0.9808 - val_loss: 0.1518 - val_accuracy: 0.9590\n",
            "Epoch 45/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0555 - accuracy: 0.9789 - val_loss: 0.1528 - val_accuracy: 0.9630\n",
            "Epoch 46/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0570 - accuracy: 0.9785 - val_loss: 0.1685 - val_accuracy: 0.9616\n",
            "Epoch 47/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0629 - accuracy: 0.9769 - val_loss: 0.1482 - val_accuracy: 0.9590\n",
            "Epoch 48/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0613 - accuracy: 0.9767 - val_loss: 0.1388 - val_accuracy: 0.9616\n",
            "Epoch 49/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 0.0514 - accuracy: 0.9828 - val_loss: 0.1378 - val_accuracy: 0.9603\n",
            "Epoch 50/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 0.0533 - accuracy: 0.9804 - val_loss: 0.1580 - val_accuracy: 0.9563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f12442c9f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unDN3c9e5Ixz",
        "outputId": "c5d580a5-7cec-4a1f-dc9e-c05fa4e98790"
      },
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkK_f2zp5Peq"
      },
      "source": [
        "submission = pd.read_excel('/content/Sample_submission.xlsx')\n",
        "submission['SECTION'] = y_pred\n",
        "submission.to_excel('/content/DeepLearn_w2v_CNN.xlsx',index=False)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoe5x_hHxOEv"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKFZ34ge9sV-"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(norm_train_reviews)\n",
        "t.word_index['<PAD>'] = 0"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8chAyxV9wso"
      },
      "source": [
        "VOCAB_SIZE = len(t.word_index)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSOFN9wT928F"
      },
      "source": [
        "train_sequences = t.texts_to_sequences(norm_train_reviews)\n",
        "test_sequences = t.texts_to_sequences(norm_test_reviews)\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=1000)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=1000)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fUxQ5uM96zR",
        "outputId": "5a05a68b-29ae-49be-d4b1-a4113eb925fd"
      },
      "source": [
        "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
        "LSTM_DIM = 128 # total LSTM units\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=1000))\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
        "model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(4, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 300)         12036900  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 1000, 300)         0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 12,290,600\n",
            "Trainable params: 12,290,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "uc7Qi5K-9-9J",
        "outputId": "d7a8a9c6-330c-48de-ef73-e7b4ddd6465b"
      },
      "source": [
        "batch_size = 100\n",
        "model.fit(X_train, to_categorical(train_sentiments), epochs=10, batch_size=batch_size, \n",
        "          shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "68/68 [==============================] - 343s 5s/step - loss: 1.1441 - accuracy: 0.4581 - val_loss: 0.2362 - val_accuracy: 0.9312\n",
            "Epoch 2/10\n",
            "68/68 [==============================] - 347s 5s/step - loss: 0.1052 - accuracy: 0.9693 - val_loss: 0.1576 - val_accuracy: 0.9537\n",
            "Epoch 3/10\n",
            "68/68 [==============================] - 347s 5s/step - loss: 0.0420 - accuracy: 0.9899 - val_loss: 0.1426 - val_accuracy: 0.9643\n",
            "Epoch 4/10\n",
            "68/68 [==============================] - 334s 5s/step - loss: 0.0098 - accuracy: 0.9977 - val_loss: 0.1890 - val_accuracy: 0.9444\n",
            "Epoch 5/10\n",
            "68/68 [==============================] - 330s 5s/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.1671 - val_accuracy: 0.9563\n",
            "Epoch 6/10\n",
            "68/68 [==============================] - 334s 5s/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.1966 - val_accuracy: 0.9497\n",
            "Epoch 7/10\n",
            "61/68 [=========================>....] - ETA: 33s - loss: 0.0012 - accuracy: 0.9999"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-1c93d8b51d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train, to_categorical(train_sentiments), epochs=10, batch_size=batch_size, \n\u001b[0;32m----> 3\u001b[0;31m           shuffle=True, validation_split=0.1, verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVGuPshZ-Q11",
        "outputId": "db87a6c3-0778-4bc0-9b47-601ce52f1e8a"
      },
      "source": [
        "predictions = model.predict_classes(X_test)\n",
        "predictions[:10]"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 1, 0, 1, 1, 1, 2, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm4osmt9-TZp"
      },
      "source": [
        "submission = pd.read_excel('/content/Sample_submission.xlsx')\n",
        "submission['SECTION'] = predictions\n",
        "submission.to_excel('/content/_LSTM.xlsx',index=False)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXiEZNdeIou3"
      },
      "source": [
        "# GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBwYiK2iImXK",
        "outputId": "cb570a7e-0d1a-44a3-923d-4549f9859a0f"
      },
      "source": [
        "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
        "GRU_DIM = 128 # total LSTM units\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=1000))\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
        "model.add(tf.keras.layers.GRU(GRU_DIM, return_sequences=False))\n",
        "model.add(tf.keras.layers.Dense(256, activation='elu'))\n",
        "model.add(tf.keras.layers.Dense(4, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1000, 300)         12036900  \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 1000, 300)         0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 128)               165120    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 12,236,072\n",
            "Trainable params: 12,236,072\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL_-jAXhJB4S",
        "outputId": "18612dcf-c54b-4748-a2ea-44568094f81f"
      },
      "source": [
        "batch_size = 100\n",
        "model.fit(X_train, to_categorical(train_sentiments), epochs=5, batch_size=batch_size, \n",
        "          shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "68/68 [==============================] - 259s 4s/step - loss: 0.8528 - accuracy: 0.6437 - val_loss: 0.4380 - val_accuracy: 0.8519\n",
            "Epoch 2/5\n",
            "68/68 [==============================] - 258s 4s/step - loss: 0.1416 - accuracy: 0.9572 - val_loss: 0.2050 - val_accuracy: 0.9378\n",
            "Epoch 3/5\n",
            "68/68 [==============================] - 260s 4s/step - loss: 0.0249 - accuracy: 0.9938 - val_loss: 0.1712 - val_accuracy: 0.9418\n",
            "Epoch 4/5\n",
            "68/68 [==============================] - 261s 4s/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.1810 - val_accuracy: 0.9418\n",
            "Epoch 5/5\n",
            "68/68 [==============================] - 263s 4s/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.1737 - val_accuracy: 0.9537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1243420c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggZbGbGJJV9x",
        "outputId": "68109937-032a-4953-cb8c-f45a700b9f5a"
      },
      "source": [
        "predictions = model.predict_classes(X_test)\n",
        "predictions[:10]"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 1, 0, 1, 1, 1, 2, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAvW-y_QJavE"
      },
      "source": [
        "submission = pd.read_excel('/content/Sample_submission.xlsx')\n",
        "submission['SECTION'] = predictions\n",
        "submission.to_excel('/content/GRU.xlsx',index=False)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okg-_M7vPBwk"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxe470xpKdZD"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upIJBF2pKcRy"
      },
      "source": [
        "\n",
        "EMBED_SIZE = 300\n",
        "EPOCHS=2\n",
        "BATCH_SIZE=128"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7yjB5aWKmj1",
        "outputId": "6cc1425d-572d-4e2b-f6e9-9b7a33c437bf"
      },
      "source": [
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=1000))\n",
        "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 1000, 300)         12036900  \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1000, 128)         153728    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 500, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 500, 64)           32832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 250, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 250, 32)           8224      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 125, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4000)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 256)               1024256   \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 13,256,197\n",
            "Trainable params: 13,256,197\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zhxn47YWhw1",
        "outputId": "b7c38b97-088b-48e5-f882-c3dc5ca44c63"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,   121,    20,  1118],\n",
              "       [    0,     0,     0, ...,  3456,  6825, 13056],\n",
              "       [    0,     0,     0, ...,   318,     5, 22292],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,     4,  2020,  2860],\n",
              "       [    0,     0,     0, ...,     9,    11,   683],\n",
              "       [    0,     0,     0, ...,     4, 20650, 13299]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHeCQJ0iWv2k",
        "outputId": "fa67152f-211d-4c8e-8f72-5934123a8047"
      },
      "source": [
        "to_categorical(train_sentiments)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVgmDPOFKl0A"
      },
      "source": [
        "# \n",
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "mdmPuVWRKs8N",
        "outputId": "f6715ac9-2b72-4da3-a482-b43cb9e209cd"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, to_categorical(train_sentiments), \n",
        "          validation_split=0.1,\n",
        "          epochs=EPOCHS, \n",
        "          batch_size=BATCH_SIZE, verbose=1)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-a1e4c2550195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           batch_size=BATCH_SIZE, verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 1) vs (None, 4))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4uppw4SK7_v"
      },
      "source": [
        "#Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "_0wPi21rK8mf",
        "outputId": "0a9cc9b2-e2c7-454c-bfea-4c19500ba2da"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-220d75bbab49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfmM6ei8LEym",
        "outputId": "8535ba38-a049-4eae-f0e9-c5b9d2aca698"
      },
      "source": [
        "predictions = model.predict_classes(X_test).ravel()\n",
        "predictions[:10]"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RUbHh1nLJZR"
      },
      "source": [
        "submission = pd.read_excel('/content/Sample_submission.xlsx')\n",
        "submission['SECTION'] = predictions\n",
        "submission.to_excel('/content/CNN.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}